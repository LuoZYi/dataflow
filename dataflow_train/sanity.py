#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
sanity.py

Sanity checks for parquet_db generated by run_parquet.py (meta.parquet + ann*.parquet).

Checks:
- meta / ann basic stats
- missing / extra slide_uids
- foreground area stats (sum area per slide)
- area bounds: 0 <= area <= rle_size_h*rle_size_w
- ROI bounds inside image: roi within (W,H)
- bbox/roi consistency (optional)
- semantic rows per slide distribution
- label distribution

Optional:
- decode COCO-style RLE (Fortran order) and visualize overlay on images
  (requires --dataset_root and pillow + matplotlib)
"""

from __future__ import annotations

import argparse
import os
import random
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd


# ---------------- RLE decode (COCO-style, Fortran order) ----------------
def rle_decode_counts(counts: List[int], h: int, w: int) -> np.ndarray:
    """
    Decode COCO RLE counts to boolean mask, with Fortran order flatten.
    counts starts with number of 0s (background).
    Returns mask (h, w) bool.
    """
    if h <= 0 or w <= 0:
        return np.zeros((max(h, 0), max(w, 0)), dtype=bool)

    total = h * w
    flat = np.zeros(total, dtype=np.uint8)
    idx = 0
    val = 0  # start with zeros
    for run in counts:
        run = int(run)
        if run < 0:
            raise ValueError(f"Negative run length in RLE: {run}")
        if run == 0:
            val = 1 - val
            continue
        end = idx + run
        if end > total:
            # clamp but keep signal that it was malformed
            end = total
        if val == 1:
            flat[idx:end] = 1
        idx = end
        val = 1 - val
        if idx >= total:
            break

    mask = flat.reshape((h, w), order="F").astype(bool)
    return mask


def safe_percentiles(x: np.ndarray, ps: Iterable[float]) -> np.ndarray:
    x = np.asarray(x)
    if x.size == 0:
        return np.array([np.nan for _ in ps], dtype=float)
    return np.percentile(x, list(ps))


def pick_existing_ann_file(ds_dir: Path, prefer: str = "semantic") -> Tuple[Path, str]:
    """
    prefer: "semantic" or "mixed"
    Returns (path, mode):
      mode = "semantic_file" if ann_semantic.parquet exists
      mode = "mixed_file" if ann.parquet exists (needs filtering by ann_kind)
      mode = "instance_file" if only ann_instance.parquet exists
    """
    p_sem = ds_dir / "ann_semantic.parquet"
    p_mix = ds_dir / "ann.parquet"
    p_ins = ds_dir / "ann_instance.parquet"

    if prefer == "semantic" and p_sem.exists():
        return p_sem, "semantic_file"
    if p_mix.exists():
        return p_mix, "mixed_file"
    if p_sem.exists():
        return p_sem, "semantic_file"
    if p_ins.exists():
        return p_ins, "instance_file"
    raise FileNotFoundError(
        f"No ann parquet found under {ds_dir}. Tried: ann.parquet / ann_semantic.parquet / ann_instance.parquet"
    )


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--db", type=str, required=True, help="parquet_db root, e.g. /home/path_sam3/dataflow/parquet_db/v1")
    ap.add_argument("--ds", type=str, required=True, help="dataset subfolder, e.g. crag")
    ap.add_argument("--prefer", type=str, default="semantic", choices=["semantic", "mixed"],
                    help="Prefer ann_semantic.parquet if available, else fall back to ann.parquet")
    ap.add_argument("--max_rows", type=int, default=0,
                    help="If >0, limit ann rows read (debug). 0 = all")
    ap.add_argument("--seed", type=int, default=0, help="Random seed for sampling")

    # optional viz
    ap.add_argument("--viz", type=int, default=0, help="If >0, visualize N sampled overlays (requires dataset_root)")
    ap.add_argument("--dataset_root", type=str, default=None,
                    help="Raw dataset root to load images for viz; used with meta.rel_path")
    ap.add_argument("--viz_out", type=str, default="./sanity_viz_out", help="Where to save overlay PNGs")
    ap.add_argument("--viz_alpha", type=float, default=0.35, help="Mask alpha for overlay")

    args = ap.parse_args()

    random.seed(args.seed)
    np.random.seed(args.seed)

    db = Path(args.db).expanduser().resolve()
    ds = args.ds
    ds_dir = db / ds
    if not ds_dir.exists():
        raise FileNotFoundError(f"Dataset folder not found: {ds_dir}")

    meta_path = ds_dir / "meta.parquet"
    if not meta_path.exists():
        raise FileNotFoundError(f"meta.parquet not found: {meta_path}")

    ann_path, ann_mode = pick_existing_ann_file(ds_dir, prefer=args.prefer)

    # ----- read meta -----
    meta_cols = ["slide_uid", "height_px", "width_px"]
    # try read rel_path for viz if present
    try:
        meta = pd.read_parquet(meta_path, columns=meta_cols + ["rel_path"])
        has_rel = True
    except Exception:
        meta = pd.read_parquet(meta_path, columns=meta_cols)
        has_rel = False

    meta["height_px"] = meta["height_px"].astype("int64")
    meta["width_px"] = meta["width_px"].astype("int64")

    print("========== META ==========")
    print("meta path:", meta_path)
    print("meta rows:", len(meta))
    n_unique_meta = meta["slide_uid"].nunique(dropna=False)
    print("meta unique slide_uid:", n_unique_meta)
    if n_unique_meta != len(meta):
        dup = meta["slide_uid"][meta["slide_uid"].duplicated()].head(10).tolist()
        print("[WARN] meta has duplicated slide_uid (show up to 10):", dup)

    # index for later alignment
    meta_idx = meta.set_index("slide_uid", drop=False)

    # ----- read ann -----
    # keep columns minimal but enough for checks
    ann_cols = ["slide_uid", "area", "ann_kind", "label_id", "label_name",
                "roi_x", "roi_y", "roi_w", "roi_h",
                "rle_size_h", "rle_size_w", "rle_counts", "rel_path"]
    # ann parquet may not contain rel_path; meta contains rel_path
    ann_cols = [c for c in ann_cols if c != "rel_path"]

    print("\n========== ANN ==========")
    print("ann path:", ann_path, "mode:", ann_mode)

    ann = pd.read_parquet(ann_path, columns=[c for c in ann_cols if c in pd.read_parquet(ann_path, columns=[]).columns] if False else ann_cols)  # fallback handled below
    # The above line isn't reliable across pyarrow versions; re-read robustly:
    try:
        ann = pd.read_parquet(ann_path, columns=ann_cols)
    except Exception:
        # fall back: read all and then keep what exists
        ann_all = pd.read_parquet(ann_path)
        keep = [c for c in ann_cols if c in ann_all.columns]
        ann = ann_all[keep].copy()

    if args.max_rows > 0 and len(ann) > args.max_rows:
        ann = ann.iloc[: args.max_rows].copy()
        print(f"[INFO] limited ann rows to {len(ann)} due to --max_rows")

    print("ann rows:", len(ann))
    if "slide_uid" not in ann.columns or "area" not in ann.columns:
        raise ValueError("ann parquet must contain at least slide_uid + area columns")

    # if mixed file, keep semantic only by default (since your sanity expects semantic)
    if ann_mode == "mixed_file" and "ann_kind" in ann.columns:
        sem = ann[ann["ann_kind"].astype(str) == "semantic"].copy()
        print("ann_kind=semantic rows:", len(sem), f"(from total {len(ann)})")
        ann = sem

    # basic sanity
    ann["area"] = ann["area"].fillna(0).astype("int64")

    n_unique_ann = ann["slide_uid"].nunique(dropna=False)
    print("ann unique slide_uid:", n_unique_ann)

    meta_uids = set(meta["slide_uid"].tolist())
    ann_uids = set(ann["slide_uid"].tolist())

    missing_ann = meta_uids - ann_uids
    extra_ann = ann_uids - meta_uids
    print("slides missing semantic anns:", len(missing_ann))
    print("anns referencing unknown slides:", len(extra_ann))
    if len(missing_ann) > 0:
        print("  (show up to 5 missing):", list(sorted(missing_ann))[:5])
    if len(extra_ann) > 0:
        print("  (show up to 5 extra):", list(sorted(extra_ann))[:5])

    # ----- semantic rows per slide -----
    rows_per_slide = ann.groupby("slide_uid").size()
    rps = rows_per_slide.to_numpy()
    print("\n========== PER-SLIDE SEMANTIC ROWS ==========")
    print("rows/slide percentiles:", safe_percentiles(rps, [0, 25, 50, 75, 90, 99]).tolist())
    print("rows/slide min/max:", int(rps.min()) if rps.size else 0, int(rps.max()) if rps.size else 0)

    # ----- label distribution (if present) -----
    if "label_name" in ann.columns:
        print("\n========== LABELS (top 15) ==========")
        vc = ann["label_name"].astype(str).value_counts().head(15)
        for k, v in vc.items():
            print(f"{k:>20s} : {int(v)}")

    # ----- area per slide + ratio checks -----
    fg = ann.groupby("slide_uid")["area"].sum()
    fg_aligned = fg.reindex(meta_idx.index).fillna(0).astype("int64")
    tot = (meta_idx["height_px"].astype("int64") * meta_idx["width_px"].astype("int64")).astype("int64")
    ratio = (fg_aligned / tot.replace(0, np.nan)).fillna(0.0).to_numpy()
    fg_np = fg_aligned.to_numpy()

    print("\n========== FOREGROUND AREA ==========")
    print("num slides:", len(fg_np))
    print("empty gt ratio:", float((fg_np == 0).mean()))
    print("area px percentiles:", safe_percentiles(fg_np, [0, 25, 50, 75, 90, 99]).tolist())
    print("coverage ratio percentiles:", safe_percentiles(ratio, [0, 25, 50, 75, 90, 99]).tolist())
    print("max coverage ratio:", float(np.max(ratio)) if ratio.size else 0.0)

    bad_area_over_img = (fg_aligned > tot)
    n_bad = int(bad_area_over_img.sum())
    print("slides with fg_area > H*W:", n_bad)
    if n_bad > 0:
        bad_ids = meta_idx.index[bad_area_over_img].tolist()[:10]
        print("  (show up to 10):", bad_ids)

    # ----- per-row bounds checks -----
    print("\n========== PER-ROW BOUNDS CHECKS ==========")

    # area <= rle_size_h*rle_size_w if present
    if "rle_size_h" in ann.columns and "rle_size_w" in ann.columns:
        rle_cap = (ann["rle_size_h"].fillna(-1).astype("int64") * ann["rle_size_w"].fillna(-1).astype("int64"))
        bad_rle_area = (ann["area"] > rle_cap) & (rle_cap > 0)
        print("rows with area > rle_size_h*rle_size_w:", int(bad_rle_area.sum()))
        if int(bad_rle_area.sum()) > 0:
            print(ann.loc[bad_rle_area, ["slide_uid", "area", "rle_size_h", "rle_size_w"]].head(5).to_string(index=False))

    # roi bounds inside image
    needed_roi = all(c in ann.columns for c in ["roi_x", "roi_y", "roi_w", "roi_h"])
    if needed_roi:
        # join H/W
        tmp = ann[["slide_uid", "roi_x", "roi_y", "roi_w", "roi_h"]].copy()
        tmp = tmp.merge(meta[["slide_uid", "height_px", "width_px"]], on="slide_uid", how="left", validate="many_to_one")

        # unknown slide_uid already counted above; mark as bad
        bad_unknown = tmp["height_px"].isna() | tmp["width_px"].isna()
        if bad_unknown.any():
            print("rows with unknown slide_uid (no meta):", int(bad_unknown.sum()))

        x0 = tmp["roi_x"].fillna(-1).astype("int64")
        y0 = tmp["roi_y"].fillna(-1).astype("int64")
        w = tmp["roi_w"].fillna(-1).astype("int64")
        h = tmp["roi_h"].fillna(-1).astype("int64")
        W = tmp["width_px"].fillna(-1).astype("int64")
        H = tmp["height_px"].fillna(-1).astype("int64")

        bad_roi = (x0 < 0) | (y0 < 0) | (w <= 0) | (h <= 0) | (x0 + w > W) | (y0 + h > H)
        print("rows with ROI out of bounds:", int(bad_roi.sum()))
        if int(bad_roi.sum()) > 0:
            print(tmp.loc[bad_roi, ["slide_uid", "roi_x", "roi_y", "roi_w", "roi_h", "width_px", "height_px"]]
                  .head(5).to_string(index=False))

        # rle_size should match roi_w/roi_h if bbox-ROI encoding
        if "rle_size_h" in ann.columns and "rle_size_w" in ann.columns:
            rsh = ann["rle_size_h"].fillna(-1).astype("int64")
            rsw = ann["rle_size_w"].fillna(-1).astype("int64")
            bad_rle_roi = (rsh != h.values) | (rsw != w.values)
            # If full-image RLE, this check will fail; so only warn if it's large portion.
            frac_bad = float(bad_rle_roi.mean()) if len(bad_rle_roi) else 0.0
            if frac_bad < 0.05:
                print("rows with rle_size != roi_size (likely OK if full-image RLE):", int(bad_rle_roi.sum()))
            else:
                print(f"[INFO] rle_size != roi_size for {frac_bad*100:.1f}% rows. "
                      f"If you used --rle_roi full, ignore this.")

    # ----- Top-K extremes -----
    print("\n========== EXTREMES ==========")
    # top 5 coverage
    order = np.argsort(-ratio)
    topk = order[:5].tolist()
    top_rows = []
    for i in topk:
        uid = meta_idx["slide_uid"].iloc[i]
        top_rows.append((uid, int(fg_np[i]), int(tot.iloc[i]), float(ratio[i])))
    for uid, a, t, r in top_rows:
        print(f"top coverage: {uid} fg={a} tot={t} ratio={r:.4f}")

    # ----- Optional visualization -----
    if args.viz > 0:
        if args.dataset_root is None:
            print("\n[VIZ] --dataset_root not provided; skip visualization.")
        elif not has_rel:
            print("\n[VIZ] meta.parquet has no rel_path column; cannot locate images; skip visualization.")
        else:
            try:
                from PIL import Image
                import matplotlib.pyplot as plt
            except Exception as e:
                print("\n[VIZ] Missing pillow/matplotlib; skip visualization. Error:", repr(e))
            else:
                dataset_root = Path(args.dataset_root).expanduser().resolve()
                out_dir = Path(args.viz_out).expanduser().resolve()
                out_dir.mkdir(parents=True, exist_ok=True)

                # pick slides: 2 extreme high ratio + 2 low + random
                uids = meta_idx.index.tolist()
                pick = []
                if len(uids) > 0:
                    pick += [meta_idx.index[order[0]], meta_idx.index[order[1]]] if len(uids) > 2 else [meta_idx.index[order[0]]]
                    pick += [meta_idx.index[np.argsort(ratio)[0]], meta_idx.index[np.argsort(ratio)[1]]] if len(uids) > 2 else []
                    remain = [u for u in uids if u not in pick]
                    random.shuffle(remain)
                    pick += remain[: max(0, args.viz - len(pick))]
                    pick = pick[: args.viz]

                # build ann subtable per slide for decode
                # Need columns for decode: roi + rle sizes + rle counts
                need_decode = all(c in ann.columns for c in ["roi_x", "roi_y", "roi_w", "roi_h", "rle_size_h", "rle_size_w", "rle_counts"])
                if not need_decode:
                    print("\n[VIZ] ann parquet lacks ROI/RLE columns required to decode; skip visualization.")
                else:
                    for uid in pick:
                        rowm = meta_idx.loc[uid]
                        rel = str(rowm["rel_path"])
                        img_path = dataset_root / rel
                        if not img_path.exists():
                            print(f"[VIZ] image not found: {img_path}")
                            continue

                        # read image
                        im = Image.open(str(img_path)).convert("RGB")
                        img = np.array(im)

                        H = int(rowm["height_px"])
                        W = int(rowm["width_px"])
                        if img.shape[0] != H or img.shape[1] != W:
                            # tolerate mismatch but warn
                            print(f"[VIZ] warn: image shape {img.shape[:2]} != meta (H,W)=({H},{W}) for {uid}")

                        # get semantic rows for this slide
                        sub = ann[ann["slide_uid"] == uid]
                        if len(sub) == 0:
                            print(f"[VIZ] no semantic rows for {uid}")
                            continue

                        # union full mask for viz
                        full = np.zeros((img.shape[0], img.shape[1]), dtype=bool)

                        for _, r in sub.iterrows():
                            roi_x = int(r["roi_x"])
                            roi_y = int(r["roi_y"])
                            roi_w = int(r["roi_w"])
                            roi_h = int(r["roi_h"])
                            rle_h = int(r["rle_size_h"])
                            rle_w = int(r["rle_size_w"])
                            counts = r["rle_counts"]
                            if counts is None:
                                continue
                            try:
                                counts_list = list(counts)
                            except Exception:
                                continue

                            m = rle_decode_counts(counts_list, rle_h, rle_w)
                            # place into full using roi
                            y1 = min(full.shape[0], roi_y + roi_h)
                            x1 = min(full.shape[1], roi_x + roi_w)
                            mh = y1 - roi_y
                            mw = x1 - roi_x
                            if mh <= 0 or mw <= 0:
                                continue
                            full[roi_y:y1, roi_x:x1] |= m[:mh, :mw]

                        # overlay
                        plt.figure()
                        plt.imshow(img)
                        plt.imshow(full.astype(float), alpha=float(args.viz_alpha))
                        plt.axis("off")
                        plt.title(f"{uid}\ncoverage={float((full.sum() / (img.shape[0]*img.shape[1]))):.3f}")

                        out_png = out_dir / f"{ds}_{uid.replace(':','_')}.png"
                        plt.savefig(str(out_png), dpi=150, bbox_inches="tight")
                        plt.close()
                        print("[VIZ] saved:", out_png)

    print("\n========== DONE ==========")


if __name__ == "__main__":
    main()

